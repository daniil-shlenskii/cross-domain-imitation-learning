seed: &seed 0
env_name: &env_name HalfCheetah-v5

n_iters_training: 5_000_000
batch_size: &batch_size 2048

precollect_buffer_size: &precollect_buffer_size 5_000

log_every: 1_000
save_every: 5_000

environment: &env
  _target_: gymnasium.wrappers.TimeLimit
  max_episode_steps: 1000
  env:
    _target_: gymnasium.make
    id: *env_name

replay_buffer: &buffer
  _target_: utils.utils.instantiate_jitted_fbx_buffer
  fbx_buffer_config:
    _target_: flashbax.make_item_buffer
    max_length: 1_000_000
    min_length: *precollect_buffer_size
    sample_batch_size: *batch_size
    add_batches: False

agent:
  _target_: agents.dida.InDomainEncoder.create
  seed: *seed
  encoding_dim: 17
  state_loss_scale: 0.2
  #
  target_batch_size: *batch_size
  target_random_buffer_state_path: ._tmp_archive_dir/random_buffers/HalfCheetah-v5.pickle
  #
  source_batch_size: *batch_size
  source_expert_buffer_state_path: experts/HalfCheetah-v5/archive/HalfCheetah-v5/collected_rollouts/buffer_state.pickle
  #
  target_encoder_config:
    _target_: gan.generator.Generator.create
    module_config:
      _target_: networks.dida.AffineTransform
    optimizer_config:
      transforms:
        - _target_: optax.clip
          max_delta: 1.0
        - _target_: optax.adamw
          learning_rate: 1e-4
    loss_config:
      _target_: agents.imitation_learning.dida.domain_encoder.losses.InDomainEncoderLoss
  state_discriminator_config:
    _target_: gan.discriminator.Discriminator.create
    module_config:
      _target_: networks.discriminator.Discriminator
      hidden_dims: [256, 256]
    optimizer_config:
      transforms:
        - _target_: optax.clip
          max_delta: 1.0
        - _target_: optax.adamw
          learning_rate: 5e-4
    loss_config:
      _target_: gan.losses.SoftplusLossGP
      gradient_penalaty_coef: 10.
      is_generator: False
  policy_discriminator_config:
    _target_: gan.discriminator.Discriminator.create
    module_config:
      _target_: networks.discriminator.Discriminator
      hidden_dims: [256, 256]
    optimizer_config:
      transforms:
        - _target_: optax.clip
          max_delta: 1.0
        - _target_: optax.adamw
          learning_rate: 1e-4
    loss_config:
      _target_: gan.losses.SoftplusLossGP
      gradient_penalaty_coef: 10.
      is_generator: False
